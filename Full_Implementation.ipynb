{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(attention, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.num_heads=num_heads\n",
    "        self.head_dim= embed_size // num_heads\n",
    "\n",
    "        assert(self.head_dim * num_heads == embed_size), \"Embed size must be completely divisible by number of heads\"\n",
    "\n",
    "        self.query=nn.Linear(self.head_dim,self.head_dim,bias=False)\n",
    "        self.key=nn.Linear(self.embed_size,self.head_dim,bias=False)\n",
    "        self.value=nn.Linear(self.embed_size,self.head_dim,bias=False)\n",
    "        self.output=nn.Linear(num_heads*self.head_dim,embed_size)\n",
    "\n",
    "    def forward(self,query,key,value,mask):\n",
    "        batch_size=query.shape[0]\n",
    "        q_len,k_len,v_len=query.shape[1],key.shape[1],value.shape[1]\n",
    "        print(\"\\n\\n\\nThe shapes before permutetion\")\n",
    "        query=query.reshape(batch_size,q_len,self.num_heads,self.head_dim)\n",
    "        print(\"The shape of query is \"+str(query.shape))\n",
    "        key=key.reshape(batch_size,k_len,self.num_heads,self.head_dim)\n",
    "        print(\"The shape of key is \"+str(key.shape))\n",
    "        value=value.reshape(batch_size,v_len,self.num_heads,self.head_dim)\n",
    "        print(\"The shape of value is \"+str(value.shape))\n",
    "        # Transpose to perform batch matrix multiplication\n",
    "        query = query.permute(0, 2, 1, 3)  # (batch_size, num_heads, q_len, head_dim)\n",
    "        key = key.permute(0, 2, 1, 3)  # (batch_size, num_heads, k_len, head_dim)\n",
    "        value = value.permute(0, 2, 1, 3)  # (batch_size, num_heads, v_len, head_dim)\n",
    "        print(\"\\n\\n\\nThe shapes after permutetion\")\n",
    "        print(\"The shape of query is \"+str(query.shape))\n",
    "        print(\"The shape of key is \"+str(key.shape))\n",
    "        print(\"The shape of value is \"+str(value.shape))\n",
    "        # Calculate attention scores\n",
    "        scores = torch.matmul(query, key.transpose(-2,-1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            mask1=torch.full(scores.size(),float('-inf'))\n",
    "            #print(mask1)\n",
    "            mask1=torch.triu(mask1,diagonal=1)\n",
    "            #print(mask1)\n",
    "            #print(scores)\n",
    "            scores += mask1\n",
    "            #print(scores)\n",
    "        \n",
    "        # Apply softmax to obtain attention weights\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # Apply dropout if needed (you can add this if desired)\n",
    "        # attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # Apply attention weights to the values\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        print(\"The initial shape of output is \"+str(output.shape))\n",
    "        # Reshape and concatenate heads\n",
    "        output = output.permute(0, 2, 1, 3).contiguous()  # (batch_size, q_len, num_heads, head_dim)\n",
    "        output = output.reshape(batch_size, q_len, self.num_heads * self.head_dim)\n",
    "        \n",
    "        # Linear transformation to get the final output\n",
    "        output = self.output(output)\n",
    "        print(\"The final shape of output is \"+str(output.shape))\n",
    "        return output        \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "The shapes before permutetion\n",
      "The shape of query is torch.Size([2, 3, 2, 4])\n",
      "The shape of key is torch.Size([2, 3, 2, 4])\n",
      "The shape of value is torch.Size([2, 3, 2, 4])\n",
      "\n",
      "\n",
      "\n",
      "The shapes after permutetion\n",
      "The shape of query is torch.Size([2, 2, 3, 4])\n",
      "The shape of key is torch.Size([2, 2, 3, 4])\n",
      "The shape of value is torch.Size([2, 2, 3, 4])\n",
      "The initial shape of output is torch.Size([2, 2, 3, 4])\n",
      "The final shape of output is torch.Size([2, 3, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Instantiate the Attention class\n",
    "embed_size = 8\n",
    "num_heads = 2\n",
    "attention_model = attention(embed_size, num_heads)\n",
    "\n",
    "# Define a sample input\n",
    "batch_size = 2\n",
    "q_len = 3\n",
    "k_len = 3\n",
    "v_len = 3\n",
    "\n",
    "# Create random tensors as input\n",
    "query = torch.randn(batch_size, q_len, embed_size)\n",
    "key = torch.randn(batch_size, k_len, embed_size)\n",
    "value = torch.randn(batch_size, v_len, embed_size)\n",
    "\n",
    "# Define a sample mask (you can modify this based on your use case)\n",
    "mask = 1  # Assuming a fully-connected attention\n",
    "\n",
    "# Call the forward method\n",
    "output = attention_model.forward(query, key, value, mask)\n",
    "\n",
    "# Print the output shape\n",
    "#print(\"Output Shape:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size,num_heads,dropout,foward_expansion):\n",
    "        super(TransformerBlock,self).__init__()\n",
    "        self.attention=attention(embed_size,num_heads=num_heads)\n",
    "        self.norm1=nn.LayerNorm(embed_size)\n",
    "        self.norm2=nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_foward=nn.Sequential(\n",
    "            nn.Linear(embed_size,foward_expansion*embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(foward_expansion*embed_size,embed_size)\n",
    "        )\n",
    "\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,query,key,value,mask):\n",
    "        multi_head_attention=self.attention(query,key,value,mask)\n",
    "        x=self.dropout(self.norm1(multi_head_attention+query))\n",
    "        foward=self.feed_foward(x)  \n",
    "        out=self.dropout(self.norm2(foward + x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding1(nn.Module):\n",
    "    def __init__(self, max_len, embed_size):\n",
    "        super(PositionalEncoding1, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        # Create constant positional encoding matrix\n",
    "        pe = torch.zeros(max_len, embed_size)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # Add batch dimension\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure that positional encoding has the same shape as input tensor x\n",
    "        batch_size, seq_len = x.shape[:2]\n",
    "        pe = self.pe[:, :seq_len, :].expand(batch_size, -1, -1)\n",
    "        return pe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_vocab_size,\n",
    "                 embed_size,\n",
    "                 num_layers,\n",
    "                 num_heads,\n",
    "                 device,\n",
    "                 foward_expansion,\n",
    "                 dropout,\n",
    "                 max_length \n",
    "                ):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.device=device\n",
    "        self.input_embedding=nn.Embedding(src_vocab_size,embed_size)\n",
    "        self.positional_encoding=PositionalEncoding1(max_length,embed_size)\n",
    "\n",
    "        self.layers=nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_size,num_heads,dropout,foward_expansion)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x,mask):\n",
    "        batch_size,seq_len=x.shape\n",
    "        print(\"Batch size is : \"+str(batch_size))\n",
    "        print(\"Seq length is : \"+str(seq_len)),\n",
    "\n",
    "        embedding=self.input_embedding(x)\n",
    "        #print(\"input embedding is :\\n\\n\\n \"+str(embedding)),\n",
    "        print(\"\\n\\n\\nInput embedding shape is :\\n\\n\\n \"+str(embedding.shape)),\n",
    "        positions=torch.arange(0,seq_len).expand(batch_size,seq_len).to(device=self.device)\n",
    "        #print(\"Position is :\\n\\n\\n \"+str(positions)),\n",
    "        #print(\"\\n\\n\\nPosition shape is :\\n\\n\\n \"+str(positions.shape)),\n",
    "        # Generate positional encoding dynamically based on the input sequence length\n",
    "        positional_encoding = self.positional_encoding(x)\n",
    "        #positional_encoding = positional_encoding.unsqueeze(0).expand(1,batch_size, embed_size, embed_size)\n",
    "        #print(\"Positional encoding is :\\n\\n\\n \"+str(positional_encoding)),\n",
    "        print(\"\\n\\n\\nPositional encoding shape is :\\n\\n\\n \"+str(positional_encoding.shape)),\n",
    "        \n",
    "        #out=self.dropout(self.input_embedding(x)+self.positional_encoding(positions))\n",
    "        out=self.dropout(embedding+positional_encoding)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out=layer(out,out,out,mask)\n",
    "        return out    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size is : 2\n",
      "Seq length is : 20\n",
      "\n",
      "\n",
      "\n",
      "Input embedding shape is :\n",
      "\n",
      "\n",
      " torch.Size([2, 20, 128])\n",
      "\n",
      "\n",
      "\n",
      "Positional encoding shape is :\n",
      "\n",
      "\n",
      " torch.Size([2, 20, 128])\n",
      "\n",
      "\n",
      "\n",
      "The shapes before permutetion\n",
      "The shape of query is torch.Size([2, 20, 8, 16])\n",
      "The shape of key is torch.Size([2, 20, 8, 16])\n",
      "The shape of value is torch.Size([2, 20, 8, 16])\n",
      "\n",
      "\n",
      "\n",
      "The shapes after permutetion\n",
      "The shape of query is torch.Size([2, 8, 20, 16])\n",
      "The shape of key is torch.Size([2, 8, 20, 16])\n",
      "The shape of value is torch.Size([2, 8, 20, 16])\n",
      "The initial shape of output is torch.Size([2, 8, 20, 16])\n",
      "The final shape of output is torch.Size([2, 20, 128])\n",
      "\n",
      "\n",
      "\n",
      "The shapes before permutetion\n",
      "The shape of query is torch.Size([2, 20, 8, 16])\n",
      "The shape of key is torch.Size([2, 20, 8, 16])\n",
      "The shape of value is torch.Size([2, 20, 8, 16])\n",
      "\n",
      "\n",
      "\n",
      "The shapes after permutetion\n",
      "The shape of query is torch.Size([2, 8, 20, 16])\n",
      "The shape of key is torch.Size([2, 8, 20, 16])\n",
      "The shape of value is torch.Size([2, 8, 20, 16])\n",
      "The initial shape of output is torch.Size([2, 8, 20, 16])\n",
      "The final shape of output is torch.Size([2, 20, 128])\n",
      "\n",
      "\n",
      "\n",
      "The shapes before permutetion\n",
      "The shape of query is torch.Size([2, 20, 8, 16])\n",
      "The shape of key is torch.Size([2, 20, 8, 16])\n",
      "The shape of value is torch.Size([2, 20, 8, 16])\n",
      "\n",
      "\n",
      "\n",
      "The shapes after permutetion\n",
      "The shape of query is torch.Size([2, 8, 20, 16])\n",
      "The shape of key is torch.Size([2, 8, 20, 16])\n",
      "The shape of value is torch.Size([2, 8, 20, 16])\n",
      "The initial shape of output is torch.Size([2, 8, 20, 16])\n",
      "The final shape of output is torch.Size([2, 20, 128])\n",
      "\n",
      "\n",
      "\n",
      "The shapes before permutetion\n",
      "The shape of query is torch.Size([2, 20, 8, 16])\n",
      "The shape of key is torch.Size([2, 20, 8, 16])\n",
      "The shape of value is torch.Size([2, 20, 8, 16])\n",
      "\n",
      "\n",
      "\n",
      "The shapes after permutetion\n",
      "The shape of query is torch.Size([2, 8, 20, 16])\n",
      "The shape of key is torch.Size([2, 8, 20, 16])\n",
      "The shape of value is torch.Size([2, 8, 20, 16])\n",
      "The initial shape of output is torch.Size([2, 8, 20, 16])\n",
      "The final shape of output is torch.Size([2, 20, 128])\n",
      "\n",
      "\n",
      "\n",
      "The shapes before permutetion\n",
      "The shape of query is torch.Size([2, 20, 8, 16])\n",
      "The shape of key is torch.Size([2, 20, 8, 16])\n",
      "The shape of value is torch.Size([2, 20, 8, 16])\n",
      "\n",
      "\n",
      "\n",
      "The shapes after permutetion\n",
      "The shape of query is torch.Size([2, 8, 20, 16])\n",
      "The shape of key is torch.Size([2, 8, 20, 16])\n",
      "The shape of value is torch.Size([2, 8, 20, 16])\n",
      "The initial shape of output is torch.Size([2, 8, 20, 16])\n",
      "The final shape of output is torch.Size([2, 20, 128])\n",
      "\n",
      "\n",
      "\n",
      "The shapes before permutetion\n",
      "The shape of query is torch.Size([2, 20, 8, 16])\n",
      "The shape of key is torch.Size([2, 20, 8, 16])\n",
      "The shape of value is torch.Size([2, 20, 8, 16])\n",
      "\n",
      "\n",
      "\n",
      "The shapes after permutetion\n",
      "The shape of query is torch.Size([2, 8, 20, 16])\n",
      "The shape of key is torch.Size([2, 8, 20, 16])\n",
      "The shape of value is torch.Size([2, 8, 20, 16])\n",
      "The initial shape of output is torch.Size([2, 8, 20, 16])\n",
      "The final shape of output is torch.Size([2, 20, 128])\n",
      "\n",
      "\n",
      "\n",
      "The shapes before permutetion\n",
      "The shape of query is torch.Size([2, 20, 8, 16])\n",
      "The shape of key is torch.Size([2, 20, 8, 16])\n",
      "The shape of value is torch.Size([2, 20, 8, 16])\n",
      "\n",
      "\n",
      "\n",
      "The shapes after permutetion\n",
      "The shape of query is torch.Size([2, 8, 20, 16])\n",
      "The shape of key is torch.Size([2, 8, 20, 16])\n",
      "The shape of value is torch.Size([2, 8, 20, 16])\n",
      "The initial shape of output is torch.Size([2, 8, 20, 16])\n",
      "The final shape of output is torch.Size([2, 20, 128])\n",
      "\n",
      "\n",
      "\n",
      "The shapes before permutetion\n",
      "The shape of query is torch.Size([2, 20, 8, 16])\n",
      "The shape of key is torch.Size([2, 20, 8, 16])\n",
      "The shape of value is torch.Size([2, 20, 8, 16])\n",
      "\n",
      "\n",
      "\n",
      "The shapes after permutetion\n",
      "The shape of query is torch.Size([2, 8, 20, 16])\n",
      "The shape of key is torch.Size([2, 8, 20, 16])\n",
      "The shape of value is torch.Size([2, 8, 20, 16])\n",
      "The initial shape of output is torch.Size([2, 8, 20, 16])\n",
      "The final shape of output is torch.Size([2, 20, 128])\n",
      "Encoder output shape: torch.Size([2, 20, 128])\n"
     ]
    }
   ],
   "source": [
    "# Toy dataset (replace with your dataset)\n",
    "src_vocab_size = 200  # Example vocabulary size\n",
    "max_length = 20  # Example maximum sequence length\n",
    "num_layers = 8\n",
    "embed_size = 128\n",
    "num_heads = 8\n",
    "forward_expansion = 4\n",
    "dropout = 0.2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the encoder\n",
    "encoder = Encoder(src_vocab_size, embed_size, num_layers, num_heads, device, forward_expansion, dropout, max_length)\n",
    "\n",
    "# Generate some sample input data\n",
    "sample_input = torch.randint(0, src_vocab_size, (2, max_length))  # Batch size 2, sequence length 20\n",
    "sample_mask = torch.ones_like(sample_input)\n",
    "\n",
    "# Pass the input data through the encoder\n",
    "encoder_output = encoder(sample_input, sample_mask)\n",
    "\n",
    "# Check the output shape\n",
    "print(\"Encoder output shape:\", encoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
