{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l=4 as it is length of sequence . (My name is hamza) is the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d_k and d_v are dimentions initialized to 8 for illusrtrative purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, d_k, d_v = 4,8,8\n",
    "q=np.random.randn(l,d_k)\n",
    "k=np.random.randn(l,d_k)\n",
    "v=np.random.randn(l,d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[-1.34436355 -1.16481997 -0.29727278  0.25227623 -0.47776357 -0.9241674\n",
      "  -0.21608085  0.60007371]\n",
      " [ 0.0847631  -2.24225177  0.52292521 -0.51078895 -0.10943865 -0.29421591\n",
      "  -0.63047738 -1.13962976]\n",
      " [ 0.03225514 -1.19766006  2.1408929   1.44447214  1.73850659  0.29836894\n",
      "   0.25893974  0.19151502]\n",
      " [ 0.40641467  2.45809268  1.04535716 -0.90538055  0.78173473 -0.59223458\n",
      "  -0.86198921 -0.665741  ]]\n",
      "K\n",
      " [[ 2.59686878e-01 -1.89382883e-01  1.07134517e+00 -2.51353453e-01\n",
      "  -4.12347745e-01 -3.32750889e-01  1.66550692e+00 -3.52089610e-01]\n",
      " [ 1.11877067e+00  1.04247187e+00 -5.99701510e-01 -3.52197696e-01\n",
      "   5.46609340e-01 -4.69034308e-01  2.85501242e-01  9.40605013e-01]\n",
      " [-1.56857969e+00  8.92873423e-01  5.70303848e-01 -1.21770706e+00\n",
      "   1.69431683e-01 -1.66625678e-04  1.20256517e+00  4.08166157e-01]\n",
      " [-1.55094112e+00 -1.56360997e+00 -1.73763089e+00 -6.08824980e-02\n",
      "  -5.28029624e-01 -3.38442417e-01 -8.92652154e-01 -5.68257178e-01]]\n",
      "V\n",
      " [[ 1.01764639e+00  7.67423299e-01 -1.79409845e+00 -1.18994179e+00\n",
      "  -4.87180511e-01 -5.14411568e-01 -1.02205241e+00 -1.46378187e+00]\n",
      " [ 2.53943594e+00  1.18476516e+00 -7.90097281e-01 -6.48697172e-01\n",
      "  -6.18781897e-01 -8.25783563e-01 -1.82812716e-01 -7.25183339e-01]\n",
      " [-1.99254210e-01  7.62952240e-01 -1.38658020e+00 -1.46718162e-02\n",
      "   1.93559998e+00  2.25991200e+00 -3.36849875e-01 -6.63241794e-01]\n",
      " [-1.24001894e+00 -9.28626420e-05  1.70182526e-01 -5.64495078e-01\n",
      "   1.83645303e-01  2.18396141e-01 -1.34321384e+00 -7.00836419e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.57705048, -1.95384555,  0.49625441,  4.8244836 ],\n",
       "       [ 0.62949267, -3.55012064, -2.45662892,  3.86475355],\n",
       "       [ 1.71343844, -1.94067099, -0.97386782, -3.3443139 ],\n",
       "       [-0.33899972,  2.54193149,  2.08014949, -5.29971971]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selfAttention=np.matmul(q,k.T)\n",
    "selfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0532344645541447, 0.7702052189692108, 7.395079166165752)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var(), k.var(), selfAttention.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.20401815, -0.69078872,  0.17545243,  1.70571254],\n",
       "       [ 0.22255927, -1.25515719, -0.86854948,  1.36639672],\n",
       "       [ 0.60579197, -0.68613081, -0.34431427, -1.18239352],\n",
       "       [-0.1198545 ,  0.8987085 ,  0.73544391, -1.87373387]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for normalizing and stabilizing values we divide by sqrt(d_k)\n",
    "selfAttention=selfAttention/ math.sqrt(d_k)\n",
    "selfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9243848957707188"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selfAttention.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking\n",
    "\n",
    "Masking is mostly done in the decoder part to ensure that the current words dont get context from the words that will be generated in the future because that will be considered cheating lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask=np.tril(np.ones((l,l)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[mask==0]=-np.infty\n",
    "mask[mask==1]=0\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.20401815,        -inf,        -inf,        -inf],\n",
       "       [ 0.22255927, -1.25515719,        -inf,        -inf],\n",
       "       [ 0.60579197, -0.68613081, -0.34431427,        -inf],\n",
       "       [-0.1198545 ,  0.8987085 ,  0.73544391, -1.87373387]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selfAttention=selfAttention+mask\n",
    "selfAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "This is done to turn vectors into probability distribution so values add upto 1 and they are interpretable as well as stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.81422742, 0.18577258, 0.        , 0.        ],\n",
       "       [0.60188681, 0.16536359, 0.23274959, 0.        ],\n",
       "       [0.15887164, 0.43994932, 0.37367818, 0.02750086]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selfAttention=softmax(selfAttention)\n",
    "selfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.01764639,  0.7674233 , -1.79409845, -1.18994179, -0.48718051,\n",
       "        -0.51441157, -1.02205241, -1.46378187],\n",
       "       [ 1.30035316,  0.84495397, -1.60758256, -1.08939338, -0.51162844,\n",
       "        -0.57225595, -0.86614468, -1.32657052],\n",
       "       [ 0.98606185,  0.83539581, -1.53322351, -0.82689603,  0.05495859,\n",
       "         0.07982152, -0.72379211, -1.15531919],\n",
       "       [ 1.17033974,  0.92825448, -1.14608872, -0.49544852,  0.37871005,\n",
       "         0.40545756, -0.40561645, -0.81870993]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v =np.matmul(selfAttention,v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.01764639e+00,  7.67423299e-01, -1.79409845e+00,\n",
       "        -1.18994179e+00, -4.87180511e-01, -5.14411568e-01,\n",
       "        -1.02205241e+00, -1.46378187e+00],\n",
       "       [ 2.53943594e+00,  1.18476516e+00, -7.90097281e-01,\n",
       "        -6.48697172e-01, -6.18781897e-01, -8.25783563e-01,\n",
       "        -1.82812716e-01, -7.25183339e-01],\n",
       "       [-1.99254210e-01,  7.62952240e-01, -1.38658020e+00,\n",
       "        -1.46718162e-02,  1.93559998e+00,  2.25991200e+00,\n",
       "        -3.36849875e-01, -6.63241794e-01],\n",
       "       [-1.24001894e+00, -9.28626420e-05,  1.70182526e-01,\n",
       "        -5.64495078e-01,  1.83645303e-01,  2.18396141e-01,\n",
       "        -1.34321384e+00, -7.00836419e-01]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function\n",
    "the function will combine all the functionalities implemented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "  d_k = q.shape[-1]\n",
    "  scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "  if mask is not None:\n",
    "    scaled = scaled + mask\n",
    "  attention = softmax(scaled)\n",
    "  out = np.matmul(attention, v)\n",
    "  return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[-1.34436355 -1.16481997 -0.29727278  0.25227623 -0.47776357 -0.9241674\n",
      "  -0.21608085  0.60007371]\n",
      " [ 0.0847631  -2.24225177  0.52292521 -0.51078895 -0.10943865 -0.29421591\n",
      "  -0.63047738 -1.13962976]\n",
      " [ 0.03225514 -1.19766006  2.1408929   1.44447214  1.73850659  0.29836894\n",
      "   0.25893974  0.19151502]\n",
      " [ 0.40641467  2.45809268  1.04535716 -0.90538055  0.78173473 -0.59223458\n",
      "  -0.86198921 -0.665741  ]]\n",
      "K\n",
      " [[ 2.59686878e-01 -1.89382883e-01  1.07134517e+00 -2.51353453e-01\n",
      "  -4.12347745e-01 -3.32750889e-01  1.66550692e+00 -3.52089610e-01]\n",
      " [ 1.11877067e+00  1.04247187e+00 -5.99701510e-01 -3.52197696e-01\n",
      "   5.46609340e-01 -4.69034308e-01  2.85501242e-01  9.40605013e-01]\n",
      " [-1.56857969e+00  8.92873423e-01  5.70303848e-01 -1.21770706e+00\n",
      "   1.69431683e-01 -1.66625678e-04  1.20256517e+00  4.08166157e-01]\n",
      " [-1.55094112e+00 -1.56360997e+00 -1.73763089e+00 -6.08824980e-02\n",
      "  -5.28029624e-01 -3.38442417e-01 -8.92652154e-01 -5.68257178e-01]]\n",
      "V\n",
      " [[ 1.01764639e+00  7.67423299e-01 -1.79409845e+00 -1.18994179e+00\n",
      "  -4.87180511e-01 -5.14411568e-01 -1.02205241e+00 -1.46378187e+00]\n",
      " [ 2.53943594e+00  1.18476516e+00 -7.90097281e-01 -6.48697172e-01\n",
      "  -6.18781897e-01 -8.25783563e-01 -1.82812716e-01 -7.25183339e-01]\n",
      " [-1.99254210e-01  7.62952240e-01 -1.38658020e+00 -1.46718162e-02\n",
      "   1.93559998e+00  2.25991200e+00 -3.36849875e-01 -6.63241794e-01]\n",
      " [-1.24001894e+00 -9.28626420e-05  1.70182526e-01 -5.64495078e-01\n",
      "   1.83645303e-01  2.18396141e-01 -1.34321384e+00 -7.00836419e-01]]\n",
      "New V\n",
      " [[ 1.01764639  0.7674233  -1.79409845 -1.18994179 -0.48718051 -0.51441157\n",
      "  -1.02205241 -1.46378187]\n",
      " [ 1.30035316  0.84495397 -1.60758256 -1.08939338 -0.51162844 -0.57225595\n",
      "  -0.86614468 -1.32657052]\n",
      " [ 0.98606185  0.83539581 -1.53322351 -0.82689603  0.05495859  0.07982152\n",
      "  -0.72379211 -1.15531919]\n",
      " [ 1.17033974  0.92825448 -1.14608872 -0.49544852  0.37871005  0.40545756\n",
      "  -0.40561645 -0.81870993]]\n",
      "Attention\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.81422742 0.18577258 0.         0.        ]\n",
      " [0.60188681 0.16536359 0.23274959 0.        ]\n",
      " [0.15887164 0.43994932 0.37367818 0.02750086]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"New V\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
